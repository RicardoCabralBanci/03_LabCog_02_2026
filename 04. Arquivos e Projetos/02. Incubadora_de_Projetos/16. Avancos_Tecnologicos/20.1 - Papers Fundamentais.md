# 20.1 - A Biblioteca de Alexandria da IA: Papers Fundamentais

Para entender por que a IA "explodiu" agora, precisamos olhar para os documentos acadêmicos que desbloquearam cada barreira tecnológica. Aqui estão os pilares que sustentam o estado da arte em 2026.

## 1. O Gênesis (Arquitetura)
*   **"Attention Is All You Need" (Google, 2017):**
    *   *O que mudou:* Introduziu a arquitetura **Transformer**. Antes dela, IAs "liam" frases palavra por palavra (sequencial). O Transformer permitiu que a IA olhasse para a frase inteira de uma vez (paralelismo), focando ("atenção") nas palavras relevantes. Sem esse paper, não existe GPT.

## 2. A Era da Escala (Scaling Laws)
*   **"Language Models are Few-Shot Learners" (OpenAI - GPT-3, 2020):**
    *   *O que mudou:* Provou que aumentar massivamente o tamanho do modelo e dos dados não trazia apenas "mais do mesmo", mas fazia emergir habilidades novas (como programar ou traduzir) que a IA não foi explicitamente treinada para fazer.
*   **"Training Compute-Optimal Large Language Models" (DeepMind - Chinchilla, 2022):**
    *   *O que mudou:* Refinou a lei da escala. Mostrou que a maioria dos modelos era *grande demais* e *treinada de menos*. Redefiniu a proporção ideal entre parâmetros e tokens, tornando os modelos mais eficientes.

## 3. A Revolução do Acesso (Eficiência e Open Source)
*   **"LoRA: Low-Rank Adaptation of Large Language Models" (Microsoft, 2021):**
    *   *O que mudou:* Em vez de retreinar um modelo gigante (caríssimo), o LoRA permite treinar apenas uma fatia minúscula (menos de 1%) de parâmetros extras. Isso permitiu que pessoas comuns fizessem *fine-tuning* em casa.
*   **"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" (Stanford, 2022):**
    *   *O que mudou:* Uma otimização matemática pura que fez as GPUs trabalharem muito mais rápido, gastando menos memória. Essencial para as janelas de contexto gigantes (100k+ tokens) que temos hoje.
*   **"LLaMA: Open and Efficient Foundation Language Models" (Meta, 2023):**
    *   *O que mudou:* O momento "Linux" da IA. A Meta provou que modelos menores e bem treinados podiam competir com gigantes fechados. O vazamento dos pesos gerou o ecossistema Open Source atual.

## 4. O Cérebro Humano (Alinhamento e Raciocínio)
*   **"Training language models to follow instructions with human feedback" (OpenAI - InstructGPT/RLHF, 2022):**
    *   *O que mudou:* Transformou completadores de texto (que só previam a próxima palavra) em *assistentes* úteis que seguem ordens, usando Feedback Humano (RLHF).
*   **"Direct Preference Optimization: Your Language Model is Secretly a Reward Model" (Stanford - DPO, 2023):**
    *   *O que mudou:* Simplificou o alinhamento, removendo a complexidade do Reinforcement Learning. Tornou o treinamento de modelos "chat" muito mais estável e barato.
*   **"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Google, 2022) & Papers de "System 2" (2024-2025):**
    *   *O que mudou:* A descoberta de que pedir para a IA "pensar passo a passo" ou usar *tokens de raciocínio* (como o o1/DeepSeek) desbloqueia lógica complexa, matemática e programação avançada.

## 5. Mídia Generativa
*   **"High-Resolution Image Synthesis with Latent Diffusion Models" (CompVis/Stability AI, 2022):**
    *   *O que mudou:* A base do Stable Diffusion. Mostrou que é possível gerar imagens comprimindo-as para um "espaço latente" (menor), tornando a geração de imagens possível em GPUs domésticas, ao contrário do DALL-E original.
