```
# -*- coding: utf-8 -*-
import os
import pathlib
import logging
from google import genai
from google.genai import types
import argparse # Para lidar com argumentos da linha de comando
import re # Para extrair a última mensagem

# --- Configuração Inicial ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
VAULT_PATH = pathlib.Path(r"C:\Lab_Cognitivo_Script\Obsidian_S9_Fe") # Raiz do Vault
NOTE_EXTENSIONS = ['.md']
# --- Configuração da Pasta de Prompts ---
PERSONALITY_PROMPT_RELATIVE_PATH = "14. Prompts/2. Personalidades/O. Jhony Deep - Jack sparrow.md"

# --- Marcadores para o Chat ---
USER_MARKER = "**Usuário:**"
AI_MARKER = "**Jack Sparrow:**"

# --- Configuração da API Gemini ---
# ####################################################################
# ### ATENÇÃO! API KEY HARDCODED ABAIXO!                           ###
# ### ... (Avisos de segurança mantidos) ...                       ###
# ####################################################################
HARDCODED_API_KEY = "AIzaSyDWG30FYAUc6XyVCv71vnR50pjWF-aN7i8" # <--- CHAVE INSERIDA DIRETAMENTE!

# Instancia o cliente da API
client: genai.Client | None = None
# ... (Lógica de inicialização do cliente mantida) ...
if not HARDCODED_API_KEY or HARDCODED_API_KEY == "SUA_CHAVE_API_REAL_AQUI":
     logging.error("### ERRO CRÍTICO: API Key está vazia ou não foi substituída! ###")
     client = None
else:
    try:
        client = genai.Client(api_key=HARDCODED_API_KEY)
        client.models.list() # Verifica a autenticação
        logging.info("Cliente da API Gemini inicializado com sucesso (usando chave HARDCODED - NÃO SEGURO!).")
    except Exception as e:
        logging.error(f"Erro CRÍTICO ao inicializar o cliente da API Gemini com a chave fornecida: {e}.")
        client = None

# Modelo Gemini a ser usado
GEMINI_MODEL_NAME = "models/gemini-1.5-pro"

# --- Funções Auxiliares ---

def read_text_file(file_path: pathlib.Path) -> str | None:
    """Lê o conteúdo de um arquivo de texto simples."""
    # ... (código inalterado) ...
    try:
        content = file_path.read_text(encoding='utf-8')
        return content
    except FileNotFoundError:
        logging.error(f"Erro: Arquivo não encontrado {file_path}")
        return None
    except UnicodeDecodeError:
        logging.warning(f"Erro de decodificação UTF-8 no arquivo {file_path}. Tentando com latin-1...")
        try:
            content = file_path.read_text(encoding='latin-1')
            logging.info(f"Leitura bem-sucedida de {file_path.name} com latin-1.")
            return content
        except Exception as e_alt:
            logging.error(f"Erro ao ler {file_path} mesmo com codificação alternativa: {e_alt}")
            return None
    except Exception as e:
        logging.error(f"Erro inesperado ao ler o arquivo {file_path}: {e}")
        return None

def extract_last_user_message(chat_content: str) -> str | None:
    """Extrai a última mensagem do usuário baseada no marcador."""
    # ... (código inalterado) ...
    last_marker_pos = chat_content.rfind(USER_MARKER)
    if last_marker_pos == -1:
        logging.warning(f"Marcador '{USER_MARKER}' não encontrado no arquivo de chat. Usando todo o conteúdo como última mensagem.")
        return chat_content.strip()

    last_message = chat_content[last_marker_pos + len(USER_MARKER):].strip()
    ai_marker_pos = last_message.find(AI_MARKER)
    if ai_marker_pos != -1:
        last_message = last_message[:ai_marker_pos].strip()

    if not last_message:
        logging.error("Não foi possível extrair a última mensagem do usuário após o marcador.")
        return None

    logging.info(f"Última mensagem do usuário extraída (início): {last_message[:100]}...")
    return last_message

# --- Função de IA com Persona e Contagem de Tokens ---
def get_persona_response(
    genai_client: genai.Client,
    user_message: str,
    personality_prompt: str,
    chat_filename: str
) -> str | None:
    """
    Chama a API Gemini para gerar uma resposta com persona, incluindo contagem de tokens.
    """
    if not genai_client:
        logging.warning("Cliente da API Gemini não inicializado. Pulando chamada.")
        return None

    # Monta o conteúdo a ser enviado
    contents = [
        f"{personality_prompt}\n\n**Mensagem do Usuário:**\n---\n{user_message}\n---\n\n**Sua Resposta (como o personagem):**"
    ]

    # --- NOVO: Contagem de Tokens ---
    try:
        count_response = genai_client.models.count_tokens(model=GEMINI_MODEL_NAME, contents=contents)
        logging.info(f"Contagem de Tokens para esta requisição: {count_response.total_tokens}")
        # TODO: Adicionar lógica aqui para verificar se count_response.total_tokens excede o limite
        # e truncar 'contents' (provavelmente o histórico, quando implementado) se necessário.
    except Exception as e_count:
        logging.warning(f"Não foi possível contar os tokens antes de enviar: {e_count}")
    # --------------------------------

    safety_settings_types = [
        types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_MEDIUM_AND_ABOVE'),
        types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_MEDIUM_AND_ABOVE'),
        types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_MEDIUM_AND_ABOVE'),
        types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_MEDIUM_AND_ABOVE'),
    ]
    generation_config = types.GenerateContentConfig(
        safety_settings=safety_settings_types,
        # temperature=0.8
    )

    try:
        logging.info(f"Chamando a API Gemini ({GEMINI_MODEL_NAME}) para responder a '{chat_filename}' como a persona...")
        response = genai_client.models.generate_content(
            model=GEMINI_MODEL_NAME,
            contents=contents,
            config=generation_config
        )
        logging.info(f"Resposta recebida da API Gemini para '{chat_filename}'.")
        # --- NOVO: Logar tokens da resposta (se disponível) ---
        try:
            # A contagem de tokens da resposta geralmente vem em usage_metadata
            if response.usage_metadata:
                 logging.info(f"Tokens da Resposta: {response.usage_metadata.candidates_token_count}")
                 logging.info(f"Tokens Totais (Prompt + Resposta): {response.usage_metadata.total_token_count}")
            else:
                 # Tenta contar a resposta manualmente se usage_metadata não estiver presente
                 response_token_count = genai_client.models.count_tokens(model=GEMINI_MODEL_NAME, contents=response.text)
                 logging.info(f"Tokens da Resposta (contagem manual aproximada): {response_token_count.total_tokens}")

        except Exception as e_resp_count:
            logging.warning(f"Não foi possível obter/contar tokens da resposta: {e_resp_count}")
        # ----------------------------------------------------

        if response.text:
             return response.text
        else:
             logging.warning(f"Resposta da API Gemini para '{chat_filename}' não contém texto. Verifique o prompt feedback:")
             if hasattr(response, 'prompt_feedback'):
                 logging.warning(f"{response.prompt_feedback}")
             else:
                 logging.warning("Prompt feedback não disponível na resposta.")
             return None
    except Exception as e:
        logging.error(f"Erro ao chamar a API Gemini para responder como persona para '{chat_filename}': {e}")
        return None

# --- Função para anexar resposta ao arquivo de chat ---
def append_ai_response_to_chat(output_text: str, chat_file_path: pathlib.Path):
    """Anexa a resposta formatada da IA ao final do arquivo de chat existente."""
    # ... (código inalterado) ...
    try:
        cleaned_output = output_text.strip()
        formatted_output = f"\n\n{AI_MARKER}\n\n{cleaned_output}\n"
        with open(chat_file_path, 'a', encoding='utf-8') as f:
            f.write(formatted_output)
        logging.info(f"Resposta da IA anexada com sucesso em: {chat_file_path}")
    except Exception as e:
        logging.error(f"Erro ao anexar resposta da IA ao arquivo de chat {chat_file_path.name}: {e}")


# --- Lógica Principal ---
def main(args):
    """Função principal do script - Foco em chat em arquivo único com persona."""
    # ... (código inicial inalterado) ...
    logging.info(f"Iniciando script - MODO: Chat em Arquivo Único com Persona.")
    logging.info(f"Arquivo de Chat: {args.chat_file}")

    if not client:
         logging.critical("Cliente da API Gemini não inicializado. Script não pode continuar.")
         return

    chat_file_path = pathlib.Path(args.chat_file)
    personality_prompt_path = (VAULT_PATH / PERSONALITY_PROMPT_RELATIVE_PATH).resolve()

    if not chat_file_path.is_file():
        logging.critical(f"Erro: Arquivo de chat não encontrado: {chat_file_path}")
        return
    if not personality_prompt_path.is_file():
        logging.critical(f"Erro: Arquivo de prompt de personalidade não encontrado em: {personality_prompt_path}")
        return

    logging.info(f"Lendo arquivo de chat: {chat_file_path.name}")
    chat_content = read_text_file(chat_file_path)
    if chat_content is None:
        logging.error(f"Falha ao ler o arquivo de chat. Abortando.")
        return

    logging.info(f"Extraindo última mensagem do usuário...")
    last_user_message = extract_last_user_message(chat_content)
    if not last_user_message:
         logging.error("Não foi possível determinar a última mensagem do usuário no arquivo de chat.")
         return

    logging.info(f"Lendo prompt de personalidade de: {personality_prompt_path.name}")
    personality_prompt = read_text_file(personality_prompt_path)
    if personality_prompt is None:
        logging.error(f"Falha ao ler o prompt de personalidade. Abortando.")
        return

    # Processamento com IA (agora inclui contagem de tokens)
    ai_response_text = get_persona_response(client, last_user_message, personality_prompt, chat_file_path.name)

    # Anexar Resposta ao Chat
    if ai_response_text:
        append_ai_response_to_chat(ai_response_text, chat_file_path)
    else:
        logging.warning("Não foi possível gerar a resposta da IA.")

    logging.info("Script concluído.")

# --- Execução e Argumentos da Linha de Comando ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Lê a última mensagem de um arquivo de chat, aplica uma persona e anexa a resposta da IA no mesmo arquivo.")
    parser.add_argument("chat_file", help="Caminho completo para o arquivo .md de chat a ser processado.")
    args = parser.parse_args()

    try:
        main(args)
    except Exception as e:
        logging.critical(f"Erro fatal não tratado na execução principal: {e}", exc_info=True)


```