```
# -*- coding: utf-8 -*-
import os
import pathlib
import logging
from google import genai
from google.genai import types
import argparse # Para lidar com argumentos da linha de comando

# --- Configuração Inicial ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
VAULT_PATH = pathlib.Path(r"C:\Lab_Cognitivo_Script\Obsidian_S9_Fe") # Raiz do Vault

# --- Configuração das Pastas ---
INPUT_FOLDER_RELATIVE_PATH = "16. Input"
OUTPUT_FOLDER_RELATIVE_PATH = "17. Output"
# Caminho para o prompt de personalidade
PERSONALITY_PROMPT_RELATIVE_PATH = "14. Prompts/2. Personalidades/O. Jhony Deep - Jack sparrow.md"

# --- Configuração da API Gemini ---
# ####################################################################
# ### ATENÇÃO! API KEY HARDCODED ABAIXO!                           ###
# ### ... (Avisos de segurança mantidos) ...                       ###
# ####################################################################
HARDCODED_API_KEY = "AIzaSyDWG30FYAUc6XyVCv71vnR50pjWF-aN7i8" # <--- CHAVE INSERIDA DIRETAMENTE!

# Instancia o cliente da API
client: genai.Client | None = None
# ... (Lógica de inicialização do cliente mantida) ...
if not HARDCODED_API_KEY or HARDCODED_API_KEY == "SUA_CHAVE_API_REAL_AQUI":
     logging.error("### ERRO CRÍTICO: API Key está vazia ou não foi substituída! ###")
     client = None
else:
    try:
        client = genai.Client(api_key=HARDCODED_API_KEY)
        client.models.list() # Verifica a autenticação
        logging.info("Cliente da API Gemini inicializado com sucesso (usando chave HARDCODED - NÃO SEGURO!).")
    except Exception as e:
        logging.error(f"Erro CRÍTICO ao inicializar o cliente da API Gemini com a chave fornecida: {e}.")
        client = None

# Modelo Gemini a ser usado - ATUALIZADO CONFORME SOLICITADO PELO USUÁRIO
# ATENÇÃO: Modelo Preview, pode não funcionar ou ter cotas restritas!
GEMINI_MODEL_NAME = "models/gemini-2.5-flash-preview-04-17" # <--- TENTANDO MODELO 2.5 FLASH PREVIEW

# --- Funções Auxiliares ---

def read_text_file(file_path: pathlib.Path) -> str | None:
    """Lê o conteúdo de um arquivo de texto simples."""
    # ... (código inalterado da v2) ...
    try:
        content = file_path.read_text(encoding='utf-8')
        return content
    except FileNotFoundError:
        logging.error(f"Erro: Arquivo não encontrado {file_path}")
        return None
    except UnicodeDecodeError:
        logging.warning(f"Erro de decodificação UTF-8 no arquivo {file_path}. Tentando com latin-1...")
        try:
            content = file_path.read_text(encoding='latin-1')
            logging.info(f"Leitura bem-sucedida de {file_path.name} com latin-1.")
            return content
        except Exception as e_alt:
            logging.error(f"Erro ao ler {file_path} mesmo com codificação alternativa: {e_alt}")
            return None
    except Exception as e:
        logging.error(f"Erro inesperado ao ler o arquivo {file_path}: {e}")
        return None

# --- Função de IA com Persona ---
def get_persona_response(
    genai_client: genai.Client,
    user_message: str,
    personality_prompt: str,
    input_filename: str # Apenas para logging
) -> str | None:
    """
    Chama a API Gemini para gerar uma resposta com base na mensagem do usuário
    e em um prompt de personalidade.
    """
    # ... (código inalterado da v2, usará o novo GEMINI_MODEL_NAME) ...
    if not genai_client:
        logging.warning("Cliente da API Gemini não inicializado. Pulando chamada.")
        return None
    contents = [
        f"{personality_prompt}\n\n**Mensagem do Usuário:**\n---\n{user_message}\n---\n\n**Sua Resposta (como o personagem):**"
    ]
    safety_settings_types = [
        types.SafetySetting(category='HARM_CATEGORY_HARASSMENT', threshold='BLOCK_MEDIUM_AND_ABOVE'),
        types.SafetySetting(category='HARM_CATEGORY_HATE_SPEECH', threshold='BLOCK_MEDIUM_AND_ABOVE'),
        types.SafetySetting(category='HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold='BLOCK_MEDIUM_AND_ABOVE'),
        types.SafetySetting(category='HARM_CATEGORY_DANGEROUS_CONTENT', threshold='BLOCK_MEDIUM_AND_ABOVE'),
    ]
    generation_config = types.GenerateContentConfig(
        safety_settings=safety_settings_types,
        # temperature=0.8
    )
    try:
        logging.info(f"Chamando a API Gemini ({GEMINI_MODEL_NAME}) para responder a '{input_filename}' como a persona...")
        response = genai_client.models.generate_content(
            model=GEMINI_MODEL_NAME, # <--- USA A VARIÁVEL ATUALIZADA
            contents=contents,
            config=generation_config
        )
        logging.info(f"Resposta recebida da API Gemini para '{input_filename}'.")
        if response.text:
             return response.text
        else:
             logging.warning(f"Resposta da API Gemini para '{input_filename}' não contém texto. Verifique o prompt feedback:")
             if hasattr(response, 'prompt_feedback'):
                 logging.warning(f"{response.prompt_feedback}")
             else:
                 logging.warning("Prompt feedback não disponível na resposta.")
             return None
    except Exception as e:
        logging.error(f"Erro ao chamar a API Gemini para responder como persona para '{input_filename}': {e}")
        return None

# --- Função para escrever resposta em novo arquivo ---
def write_ai_response(output_text: str, output_folder_path: pathlib.Path, original_filename: str):
    """
    Cria/sobrescreve um arquivo .md na pasta de saída com o mesmo nome do original
    e escreve a resposta da IA nele.
    """
    # ... (código inalterado da v2) ...
    try:
        cleaned_output = output_text.strip()
        lines = cleaned_output.splitlines()
        if lines and lines[0].strip().startswith("```"):
            lines = lines[1:]
            cleaned_output = "\n".join(lines)
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
            cleaned_output = "\n".join(lines).strip()

        output_folder_path.mkdir(parents=True, exist_ok=True)
        logging.info(f"Pasta de destino para saída verificada/criada: {output_folder_path}")
        output_filename = pathlib.Path(original_filename).with_suffix('.md').name
        output_file_path = output_folder_path / output_filename
        output_file_path.write_text(cleaned_output, encoding='utf-8')
        logging.info(f"Resposta da IA escrita com sucesso em: {output_file_path}")
    except Exception as e:
        logging.error(f"Erro ao criar ou escrever no arquivo de saída {original_filename}: {e}")


# --- Lógica Principal ---

def main(args):
    """
    Função principal do script - Foco em chat assíncrono com persona.
    """
    logging.info(f"Iniciando script - MODO: Chat Assíncrono com Persona.")
    logging.info(f"Arquivo de Input Relativo: {args.input_file_relative}")

    if not client:
         logging.critical("Cliente da API Gemini não inicializado. Script não pode continuar.")
         return

    # --- Construção e Validação dos Caminhos ---
    input_file_path = (VAULT_PATH / INPUT_FOLDER_RELATIVE_PATH / args.input_file_relative).resolve()
    personality_prompt_path = (VAULT_PATH / PERSONALITY_PROMPT_RELATIVE_PATH).resolve()
    output_folder_path = (VAULT_PATH / OUTPUT_FOLDER_RELATIVE_PATH).resolve()

    if not input_file_path.is_file():
        logging.critical(f"Erro: Arquivo de input não encontrado: {input_file_path}")
        return
    if not personality_prompt_path.is_file():
        logging.critical(f"Erro: Arquivo de prompt de personalidade não encontrado em: {personality_prompt_path}")
        logging.critical(f"Verifique se o caminho '{PERSONALITY_PROMPT_RELATIVE_PATH}' está correto no script e se o arquivo existe.")
        return

    # --- Leitura dos Arquivos ---
    logging.info(f"Lendo mensagem do usuário de: {input_file_path.name}")
    user_message = read_text_file(input_file_path)
    if user_message is None:
        logging.error(f"Falha ao ler a mensagem do usuário. Abortando.")
        return

    logging.info(f"Lendo prompt de personalidade de: {personality_prompt_path.name}")
    personality_prompt = read_text_file(personality_prompt_path)
    if personality_prompt is None:
        logging.error(f"Falha ao ler o prompt de personalidade. Abortando.")
        return

    # --- Processamento com IA ---
    ai_response_text = get_persona_response(client, user_message, personality_prompt, input_file_path.name)

    # --- Escrita do Resultado ---
    if ai_response_text:
        write_ai_response(ai_response_text, output_folder_path, input_file_path.name)
    else:
        logging.warning("Não foi possível gerar a resposta da IA.")

    logging.info("Script concluído.")

# --- Execução e Argumentos da Linha de Comando ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Lê uma mensagem de um arquivo na pasta Input, aplica uma persona de um prompt e salva a resposta da IA na pasta Output.")
    parser.add_argument("input_file_relative", help="Nome do arquivo (com extensão) dentro da pasta '16. Input' contendo a mensagem do usuário.")
    args = parser.parse_args()

    try:
        main(args)
    except Exception as e:
        logging.critical(f"Erro fatal não tratado na execução principal: {e}", exc_info=True)


```