---
data: 2026-02-06
status: üü° Planejamento
tags:
  - PLANEJAMENTO
  - ia
  - memoria
  - rag
---
# Planejamento: Sistema RAG para Mem√≥ria de Longo Prazo

## Problema

O MEMORY.md do Claude Code tem limite de 200 linhas no system prompt. Conforme o Core cresce (centenas de notas), a IA perde capacidade de manter contexto relevante entre sess√µes.

## Solu√ß√£o proposta

Pipeline de RAG (Retrieval Augmented Generation) local que:
1. Indexa todas as notas do Core como embeddings
2. Na inicializa√ß√£o de sess√£o, busca apenas o contexto relevante
3. Injeta um resumo compacto no MEMORY.md ou diretamente no prompt

## Arquitetura conceitual

```
01. Core/*.md
       ‚Üì
   Chunking (por nota ou por se√ß√£o)
       ‚Üì
   Embedding model (sentence-transformers local ou API)
       ‚Üì
   Vector DB (LanceDB / ChromaDB / FAISS)
       ‚Üì
   Query agent (busca sem√¢ntica por contexto)
       ‚Üì
   Resumo compacto ‚Üí MEMORY.md ou prompt
```

## Componentes necess√°rios

- **Indexador**: Script que l√™ o Core, faz chunking e gera embeddings. Roda ap√≥s mudan√ßas (pode integrar com vault_watcher).
- **Vector DB**: Armazenamento local dos embeddings. LanceDB √© candidato (leve, sem servidor).
- **Query agent**: Recebe o contexto da sess√£o e retorna os chunks mais relevantes.
- **Gerador de resumo**: Compacta os chunks recuperados em um resumo de poucas linhas para o MEMORY.md.

## Decis√µes em aberto

- [ ] Escolher embedding model (local vs API ‚Äî custo vs qualidade)
- [ ] Definir estrat√©gia de chunking (nota inteira? por se√ß√£o? por par√°grafo?)
- [ ] Definir trigger de re-indexa√ß√£o (a cada save? a cada sess√£o? manual?)
- [ ] Decidir se o resumo vai no MEMORY.md ou direto no prompt via script

## Pr√©-requisitos

- Estrutura b√°sica do Lab Cognitivo estabilizada
- Volume suficiente de notas no Core para justificar o investimento

## Refer√™ncias

- LanceDB j√° presente no projeto ClawdBot (`001.1 projetos/00. ClawdBot/`)
- `find_orphan_notes.py` e `scan_core.py` como base para o indexador
